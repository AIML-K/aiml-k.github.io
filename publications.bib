@inproceedings{10.1007/978-3-031-05936-0_26,
author = {Lee, Donghun and Powell, Warren B.},
title = {Online Learning with&nbsp;Regularized Knowledge Gradients},
year = {2022},
isbn = {978-3-031-05935-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-05936-0_26},
doi = {10.1007/978-3-031-05936-0_26},
abstract = {We introduce a simple and effective regularization of knowledge gradient (KG) and use it to present the first sublinear regret bound result for KG-based algorithms. We construct online learning with regularized knowledge gradients (ORKG) algorithm with independent Gaussian belief model, and prove that ORKG algorithm achieves sublinear regret upper bound with high probability facing bounded independent Gaussian multi-armed bandit (MAB) problems. The theoretical properties of regularized KG and ORKG algorithm are analyzed, and the empirical characteristics of ORKG algorithm are empirically validated with MAB benchmark simulations. ORKG algorithm shows top-tier performance comparable to select MAB algorithms with provable regret bounds.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May 16–19, 2022, Proceedings, Part II},
pages = {328–339},
numpages = {12},
keywords = {Knowledge gradient, Online learning, Regret analysis},
location = {Chengdu, China}
}